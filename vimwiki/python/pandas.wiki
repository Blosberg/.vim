import pandas as pd
# Identify data type:
type(var)

# In pandas create 1-D arrays (series) like lists

= read/ write =

# read
df = pd.read_csv( "WindowMapOutput_w200_s50_table_noNA.tsv" ,
                  sep="\t"
		  header = None,
		  index_col=0
		  )
# numpy version of this is  np.genfromtxt(datafile, skip_header=1).

# write
df.to_csv( file_name,
sep='\t'
header=None,
index=False,
float_format='%.3f'
)

# or, if you don't want 0.0001 to be rounded to zero:
df.to_csv('pandasfile.csv', float_format='%g')


# by default both index and header are added

= Series =

S = pd.Series( [1,2,3],
               index=["a","b","c"])
# You can then index, slice, etc. the same way as lists.
S.loc["a":"c"]
S.iloc[0:2]

   = processing data : value_counts() (similar to table), sorting, etc. =

# check frequencies of occurance of each index
S.value_counts()

# sort (if hierarchical, argument becomes list)
S.sort_values(["first","second",...] inplace = False )

# str. attributes are available to every Series
S.str.[contains/startswith/isnumeric]()

= Data Frames: create them and access their contents =

There is a [[numpy]] array inside every data frame.
Access it with .values
Alternative access:

# -- useful df functions --
# get dimensions with:
df.shape
# names of columns/rows:
df.columns
df.index

# get first, last n lines
df.head(n), df.tail(n)

# get set of statistical info (mean, std. dev, etc.)
df.describe()

# get Pearson correlation between cols
pd.DataFrame.corr(df)

from sklearn.cluster.bicluster import SpectralCoclustering


   = build a data frame from Series =

To create data frames from series, you can combine series, and pandas will match the keys up for you:

>>> X=pd.Series([1,2,3,4], index=["a","b","c","d"])
>>> Y=pd.Series([5,6,7,8], index=["a","c","d","e"])
>>> df=pd.DataFrame({"First":X,"Second":Y})
>>> df
   First  Second
a    1.0     5.0
b    2.0     NaN
c    3.0     6.0
d    4.0     7.0
e    NaN     8.0
# Note the missing fields in b, and e

# Add a column with a single value:
df['ColName'] = pd.Series(  'Value', index=df.index )


   = set the index (row names), colnames of the df, rename them =

# change the first column define the index of the df (previous index is lost)
df.set_index("First", inplace=True)

# reset indext to default integers
df.reset_index()

# sort it by that criteria
df.sort_index()

countries = pd.DataFrame({
'country': ['United States', 'The Netherlands', 'Spain', 'Mexico', 'Australia'],
'capital': ['Washington D.C.', 'Amsterdam', 'Madrid', 'Mexico City', 'Canberra'],
'continent': ['North America', 'Europe', 'Europe', 'North America', 'Australia'],
'language': ['English', 'Dutch', 'Spanish', 'Spanish', 'English']})

>>> countries

 	country 	capital 	continent 	language
0 	United States 	Washington D.C. 	North America 	English
1 	The Netherlands 	Amsterdam 	Europe 	Dutch
2 	Spain 	Madrid 	Europe 	Spanish
3 	Mexico 	Mexico City 	North America 	Spanish
4 	Australia 	Canberra 	Australia 	English


# get col names:
list( countries )
countries.columns

# Rename columns specifically:
countries.rename({'capital': 'capital_city', 'language': 'most_spoken_language'}, axis='columns', inplace=True)
>>> countries

 	country 	capital_city 	continent 	most_spoken_language
0 	United States 	Washington D.C. 	North America 	English
1 	The Netherlands 	Amsterdam 	Europe 	Dutch
2 	Spain 	Madrid 	Europe 	Spanish
3 	Mexico 	Mexico City 	North America 	Spanish
4 	Australia 	Canberra 	Australia 	English

# Set columns in order:
>>> countries.columns = ["a", "b", "c", "d" ]
>>> countries

 	a 	b 	c 	d
0 	United States 	Washington D.C. 	North America 	English
1 	The Netherlands 	Amsterdam 	Europe 	Dutch
2 	Spain 	Madrid 	Europe 	Spanish
3 	Mexico 	Mexico City 	North America 	Spanish
4 	Australia 	Canberra 	Australia 	English


   = Access (select)  rows and columns (+boolean logical indexing selection); index for largest/smallest, slicing=

     == Specific locations  ==

# Latter only works if the col name has no spaces.
df.loc["b":"d"]  # select columns b through.
df.iloc[2, 4]    # gets rows (or columns) at positions (only takes integers).
df.ix[2,3]       # First tries to be like loc, but if it fails,
                 # it then falls back to iloc

`df[["FirstColName","SecondColName"]]`
# n.b. if you pass an array of indices you get another df, (even if the array has only one element)
`df.loc[ : , ["FirstColName"]  ]`   #  --> df
# but with a single index you get a series.
df["First"] or df.First           #  --> series

#
# Access multiple cols by creating a list [ , ]
# within the dereferencing braces -hence, double braces


df.iloc[[0,4,5],[1]]
df.ix[0:2,0:5]
# N.B. ix will first try to match index explicitly (so if your indexes are
# named 0,1,2, then this will include all three rows.) but if your indexes
# have a different name (e.g. "first","second","third") then it wil
# automatically switch to integer indexing and will not include the last
# value (i.e. 0:2 = [0,1])

   == Logical indexing (boolean) ==

# logical indexing requires "loc" (iloc won't work)
   df.loc[ df["val"] > 0, : ]
#             ^ single index yields series, the length of which must match number of rows.
#  Here you'll get the rows for which condition is satisfied.


# select based on whether language is one thing _or_ another:
countries[ countries.language.isin(['Dutch', 'English'])]

    ==  min/max  ==
#
# Select the rows of a df based on where the value of "col_3" are the nsmallest largest
df.nsmallest(3,"col_3")
test.nlargest(2,"col_2")

# the location of largest/smallest value by index
# using idxmin() and idxmax() functions:
x=pd.Series([1,3,5,2,76,43])
x.idxmin()
x.idxmax()


df.index    # to get list a-f

   = create a "dummy" 1/0 array of columns ones and zeros for each variable now combined =

   pd.get_dummies( countries, drop_first=True)

   = clean out NaNs =

   # boolean col denoting whether the number of nans discovered in each row is 0
   Row_has_no_col_null = Prom_bc_paired.loc[:,["promoter","barcode"]].isnull().astype(int).sum(axis=1) == 0
   # filtered df
    Prom_bc_nunulls = Prom_bc_paired.loc[ Row_has_no_col_null, :]
   = Boolean indexing =
AND : &
OR  : |
NOT : ~

df[(df.First <3.5) & (df.Fourth > 4 )]
# also: df.colname.str.contains("substring")-->bool

df.loc[ boolArray, : ]
#   ^ select locations where truth is found

= Tidy, rearrange, manipulate: melt stack/unstack, groupby; combing/cat/merge columns; split columns =

   == split/ recombine columns ==
	# Starting with a bedfile:
>>> bedU_df
	chrom 	start 	end 	E135_S0-CreCtrl 	E135A_S0-T23DKO
0 	chr1 	3000826 	3000828 	0.917 	0.958
1 	chr1 	3001006 	3001008 	0.903 	0.958
2 	chr1 	3001017 	3001019 	0.893 	0.920
3 	chr1 	3001276 	3001278 	0.947 	0.905
4 	chr1 	3001628 	3001630 	0.862 	0.958

Meth_df=bedU_df.set_index( bedU_df['chrom'].astype(str)
                          +":"+ bedU_df["start"].astype(str)
                          +"-"+ bedU_df["end"].astype(str)   ).drop(["chrom","start", "end"], axis=1)

# if you have a tsv with "Range" column, you can reverse this with:
tmp=tsvU_df["Range"].str.split(r":|-", expand=True )

# rename the columns:
tmp.columns=["chr", "start", "end"]

# and then add-back together the other stuff
tmp.join( tsvU_df )

	=== Melt ===

https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html
# Melt puts a table into single-observable column-wise format
# e.g. Starting off with a metilene output file (converted to tsv)
>>> df
	                q-value  ... p_2DKS 	mean_g1 	mean_g2
location 			 ...
chr1:7621909-7622017 	0.004158 ... 6.421400e-07 	0.698760 	0.50662
chr1:9680142-9680354 	0.023021 ... 8.969600e-06 	0.194800 	0.31956
chr1:14242645-14242713 	0.007769 ... 1.750300e-06 	0.054915 	0.18143
... 	... 	... 	... 	... 	... 	... 	...


# "Melt" converts table with many columns into single-observable rows with just one value:
>>>df.loc[:, [ "mean_g1", "mean_g2"] ].melt( var_name='AgeGroup', value_name='MethFrac')

var_name='AgeGroup'   # col that will be created, populated by values to hold the previously separated columns
value_name='MethFrac' # <-- col that will be kept
id_vars   = "CGtype" # <-- cols that will be preserved

# or, alternatively:
pd.melt( df )

			AgeGroup 	MethFrac
0 	mean_g1 	0.698760
1 	mean_g1 	0.194800
2 	mean_g1 	0.054915
3 	mean_g1 	0.111400
4 	mean_g1 	0.280900
... 	... 	...


df_melted = df.melt( value_name= "MethFrac",
                     var_name  = "sample",
                     id_vars   = "CGtype" )


   == stack() ==
# Return a taller df
# stack Column subset --> into hierarchical index labels for rows (within  existing row index labels)

   == unstack() ==
# return a wider df
# unstack hierarchical index labels into Column subsets

# N.B. In both stacking, unstacking you can't go lower than the first set of
# columns and rows. This is just for pivotting subgroups within those
# columns/rows.

   == Groupby ==

# produces a set of dataframes, grouped by categorical values in a provided ColName
gb = df.groupby("ColName")
# Then gb is a set of tuples with the 1rst entry the ColName
# group label, and  the second being a dataframe (subset of the original).

# worked example:
# Starting with:

>>> Dat_VE
                          Range  Celli  MethFrac  Nhits
0          chr1:4496947-4497608      0   0.00000      6
1          chr1:6441200-6441612      0   0.33333      9
2        chr1:10993486-10994005      0   0.00000     16
3        chr1:19213940-19214404      0   0.00000      1
4        chr1:19236612-19236850      0   0.14286      7
...                         ...    ...       ...    ...
76233  chr9:114384736-114385011     85   0.50000      4
76234  chr9:121838954-121839539     85   0.00000      3
76235  chr9:122747129-122747451     85   0.00000      8
76236  chr9:122903301-122903536     85   0.00000      1
76237  chr9:123156818-123157109     85   0.66667      9

>>> Dat_VE.drop(["MethFrac"], axis=1).groupby(['Celli']).mean()
#                 # ^ drop this col
#                                     ^ take mean of other cols


for x,y in gb:
   # x is the ColName label, y is the group:
   ...

# look at groups
gb.groups
gb.get_group("Name") # specific group

# calculations on other cols within groups
gb["col"].agg(np.mean)

# batch calculations on the groupby sets:
gb.size()  # num entries in each group
gb.count() # ^ Same as above but num entries preserved in cols

gb.first()/last()     # first/last entry in each group
gb.mean()/min()/max() #


   == agg()  ==

# apply a named func to all remaining columns
  gb.agg(["min","max",...])

# Or supply a dictionary with specific funcs to each col:

  gb.agg({"Colname2":["func1","func2",...], "ColName3":... })
# OTHER cols   ^                              ^
# various funcs         ^        ^                      ^
# here, while groupby is grouped by ColName, you can operated on ColName2,3
# using functions specified in the corresponding dict.

   == concatenate  ==

 df = pd.DataFrame({'col_1': [1, 0], 'col_2': [0, 1], 'col_3': [1, 0]})
 df_1 = pd.DataFrame({'col_1': [6, 7, 8], 'col_2': [1, 2, 3], 'col_3': [5, 6, 7]})

pd.concat([df, df_1]).reset_index(drop=True)

= regression on columns =

model = LinearRegression()

x = df.colName.values
y = ( df[ "col1"]-df["col2"]).values

x = x.reshape( x.shape[0], 1)
y = y.reshape( y.shape[0], 1)

model.fit( x, y)


= Sort =
# sort by multiple hierarchical criteria:
df.sort_values( by=["most-important col",
                   "Second-most-important col",
                    ...],
               ascending = True [default] )
= TODO: =
* - Figure out "Categorical Indices" in data frames --Why can't you just add one? (see "flights" problem in 06_04 exercises)
* - How to use pd.IndexSlice to slice sub-indices (see 06_04)


